# üìò Otimiza√ß√£o de Join com PySpark

Este notebook demonstra como aplicar boas pr√°ticas de performance em opera√ß√µes de **join** utilizando o **Apache Spark** com PySpark, especialmente ao trabalhar com arquivos no formato `.parquet`.

---

## üîß Tecnologias Utilizadas

- Python 3.x
- Apache Spark (via PySpark)
- Formato de arquivos: `.parquet`

---

## üìÇ Etapas Realizadas

### ‚úÖ 1. Leitura dos Dados

```python
df_video = spark.read.parquet("videos-preparados.snappy.parquet")
df_comments = spark.read.parquet("videos-comments-tratados.snappy.parquet")
```

---

### ‚úÖ 2. Cria√ß√£o de Tabelas Tempor√°rias

```python
df_video.createOrReplaceTempView("videos")
df_comments.createOrReplaceTempView("comentarios")
```

---

### ‚úÖ 3. Join com Spark SQL

```sql
SELECT *
FROM videos v
JOIN comentarios c
ON v.`Video ID` = c.`Video ID`
```

---

### ‚úÖ 4. Otimiza√ß√£o com `repartition` e `coalesce`

```python
df_video_rep = df_video.repartition("Video ID")
df_comments_rep = df_comments.repartition("Video ID")

df_video_rep.createOrReplaceTempView("videos_rep")
df_comments_rep.createOrReplaceTempView("comentarios_rep")

join_video_comments_rep = spark.sql("""
    SELECT *
    FROM videos_rep v
    JOIN comentarios_rep c
    ON v.`Video ID` = c.`Video ID`
""")

join_video_comments_rep = join_video_comments_rep.coalesce(1)
```

---

## üí° Objetivo

O notebook tem como objetivo demonstrar como o uso de `repartition` pode melhorar o desempenho de joins distribu√≠dos e como `coalesce` ajuda a consolidar os dados ap√≥s o processamento, evitando a gera√ß√£o de m√∫ltiplos arquivos de sa√≠da.

---

## ‚ñ∂Ô∏è Como Executar

1. Instale o PySpark:
   ```bash
   pip install pyspark
   ```

2. Execute o notebook:
   - Com Jupyter Notebook:
     ```bash
     jupyter notebook otimizacao.ipynb
     ```
   - Ou em ambiente como Google Colab (subindo os arquivos `.parquet` previamente).

---

## üìù Arquivo

- `otimizacao.ipynb`: Notebook contendo todo o c√≥digo do processo descrito acima.
